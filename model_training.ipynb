{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "7lkDg-h3Jau4",
        "outputId": "83d1dac5-5382-4c64-fb78-65a85e630994"
      },
      "outputs": [],
      "source": [
        "import opendatasets as od\n",
        "od.download_kaggle_dataset('https://www.kaggle.com/datasets/grassknoted/asl-alphabet','')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "RdHr2E9aQ2zd",
        "outputId": "c529661d-7c76-42b4-97ad-f6c1cd40b485"
      },
      "outputs": [],
      "source": [
        "import subprocess\n",
        "\n",
        "def run_command(command):\n",
        "    try:\n",
        "        # Run the command and capture the output\n",
        "        result = subprocess.run(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True, shell=True)\n",
        "\n",
        "        # Check if the command was successful\n",
        "        if result.returncode == 0:\n",
        "            print(\"Command executed successfully!\")\n",
        "            print(\"Output:\\n\", result.stdout)\n",
        "        else:\n",
        "            print(\"Command failed with return code:\", result.returncode)\n",
        "            print(\"Error output:\\n\", result.stderr)\n",
        "\n",
        "        return result.stdout, result.stderr, result.returncode\n",
        "\n",
        "    except Exception as e:\n",
        "        print(\"An error occurred:\", e)\n",
        "        return None, None, -1\n",
        "\n",
        "output, error, returncode = run_command('pip install xgboost==1.7.6')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YQF2dfyIvTf-",
        "outputId": "aad21993-6076-40ef-edb4-869d00fcfa2f"
      },
      "outputs": [],
      "source": [
        "import xgboost\n",
        "xg = xgboost.XGBClassifier.__sklearn_tags__\n",
        "print(xg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "collapsed": true,
        "id": "yMhByCcNSBTK",
        "outputId": "784ecbaf-ad53-4c8f-a6a2-4929d74be7fc"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BWI3IMr2qOrY"
      },
      "source": [
        "# To process word dataset:\n",
        "Extract the csv file containing one hot encoding of classes and save corresponding image in a dataframe\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SpPUjg4uqy-3"
      },
      "outputs": [],
      "source": [
        "train_csv, test_csv, valid_csv = '/content/drive/MyDrive/ASL Dataset.v4-v1-aug-and-prep-with-validation-set.multiclass/train/_classes.csv', '/content/drive/MyDrive/ASL Dataset.v4-v1-aug-and-prep-with-validation-set.multiclass/test/_classes.csv', '/content/drive/MyDrive/ASL Dataset.v4-v1-aug-and-prep-with-validation-set.multiclass/valid/_classes.csv'\n",
        "train_dir, test_dir, valid_dir = '/content/drive/MyDrive/ASL Dataset.v4-v1-aug-and-prep-with-validation-set.multiclass/train','/content/drive/MyDrive/ASL Dataset.v4-v1-aug-and-prep-with-validation-set.multiclass/test','/content/drive/MyDrive/ASL Dataset.v4-v1-aug-and-prep-with-validation-set.multiclass/valid'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z5BZ0FP4qMBQ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "def get_class_label(row):\n",
        "    # Extract the class name where value is 1\n",
        "    classes = row.index[1:]  # Skip 'filename' column\n",
        "    return classes[row.values[1:].argmax()]\n",
        "\n",
        "# Load CSV and map filenames to class labels\n",
        "def process_roboflow_csv(csv_path, image_dir):\n",
        "    df = pd.read_csv(csv_path)\n",
        "    df[\"label\"] = df.apply(get_class_label, axis=1)\n",
        "    df[\"filepath\"] = df[\"filename\"].apply(lambda x: os.path.join(image_dir, x))\n",
        "    return df[[\"filepath\", \"label\"]]\n",
        "\n",
        "# Example usage\n",
        "train_df = process_roboflow_csv(train_csv, train_dir)\n",
        "valid_df = process_roboflow_csv(valid_csv, valid_dir)\n",
        "test_df = process_roboflow_csv(test_csv, test_dir)\n",
        "\n",
        "# Combine all splits\n",
        "roboflow_df = pd.concat([train_df, valid_df, test_df])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0XCtRtnPunyK"
      },
      "source": [
        "#Handling ASL alphabets data:\n",
        "Load the image path and corresponding class lable (directory name) in a dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eGga1CWIumoZ"
      },
      "outputs": [],
      "source": [
        "asl_dir = \"/content/asl-alphabet/asl_alphabet_train/asl_alphabet_train\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZcUxCopeud1q"
      },
      "outputs": [],
      "source": [
        "kaggle_data = []\n",
        "for class_label in os.listdir(asl_dir):\n",
        "    class_dir = os.path.join(asl_dir, class_label)\n",
        "    if not os.path.isdir(class_dir):\n",
        "        continue\n",
        "    for img_name in os.listdir(class_dir):\n",
        "        img_path = os.path.join(class_dir, img_name)\n",
        "        kaggle_data.append({\"filepath\": img_path, \"label\": class_label})\n",
        "\n",
        "kaggle_df = pd.DataFrame(kaggle_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D7ZsHGqsvrqk"
      },
      "source": [
        "# Combine both dataframes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MizMAc8FvpNE"
      },
      "outputs": [],
      "source": [
        "combined_df = pd.concat([roboflow_df, kaggle_df], ignore_index=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I0FAmZ81v_Jk"
      },
      "source": [
        "# Preprocessing\n",
        "Use mediapipe to extract landmark from images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QyTdmbMfwCqo"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import mediapipe as mp\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ccNV32LWwjjI"
      },
      "outputs": [],
      "source": [
        "mp_hands = mp.solutions.hands\n",
        "hands = mp_hands.Hands(static_image_mode=True, max_num_hands=2, min_detection_confidence=0.5)\n",
        "\n",
        "def extract_landmarks(image_path):\n",
        "    image = cv2.imread(image_path)\n",
        "    if image is None:\n",
        "        return None\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    results = hands.process(image)\n",
        "\n",
        "    landmarks = []\n",
        "    if results.multi_hand_landmarks:\n",
        "        # Use first detected hand\n",
        "        for landmark in results.multi_hand_landmarks[0].landmark:\n",
        "            landmarks.extend([landmark.x, landmark.y, landmark.z])\n",
        "    return landmarks\n",
        "\n",
        "# Process all images\n",
        "combined_df[\"landmarks\"] = combined_df[\"filepath\"].apply(extract_landmarks)\n",
        "\n",
        "# Drop rows where landmarks failed to extract\n",
        "combined_df = combined_df.dropna(subset=[\"landmarks\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zhz-KibK8DM2"
      },
      "source": [
        "#Export dataframe as CSV for further processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lKK7TzCU8CFC"
      },
      "outputs": [],
      "source": [
        "# Expand landmarks into columns\n",
        "landmark_cols = [f\"{ax}_{i}\" for i in range(21) for ax in [\"x\", \"y\", \"z\"]]\n",
        "landmark_df = pd.DataFrame(\n",
        "    combined_df[\"landmarks\"].tolist(),\n",
        "    columns=landmark_cols\n",
        ")\n",
        "\n",
        "final_df = pd.concat([combined_df[[\"label\"]], landmark_df], axis=1)\n",
        "final_df.to_csv(\"asl_landmarks_combined.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Syp-Week-keE"
      },
      "source": [
        "#Remove undetected missing values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ox4GF6Y2-jwz",
        "outputId": "961405b4-52a5-4be6-8801-8deb1035a571"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "def process_landmarks_dataset(csv_path):\n",
        "    # Load the dataset\n",
        "    df = pd.read_csv(csv_path)\n",
        "\n",
        "    # Identify landmark columns (all columns except 'label')\n",
        "    landmark_cols = df.columns.drop('label')\n",
        "\n",
        "    # Find rows where ALL landmarks are missing\n",
        "    all_missing_mask = df[landmark_cols].isna().all(axis=1)\n",
        "\n",
        "    # Create cleaned dataset\n",
        "    cleaned_df = df[~all_missing_mask].copy()\n",
        "\n",
        "    # Count records per class before/after cleaning\n",
        "    original_counts = df['label'].value_counts().rename('Original Records')\n",
        "    cleaned_counts = cleaned_df['label'].value_counts().rename('Populated Records')\n",
        "\n",
        "    # Combine counts and fill missing classes with 0\n",
        "    count_df = pd.concat([original_counts, cleaned_counts], axis=1).fillna(0)\n",
        "    count_df['Populated Records'] = count_df['Populated Records'].astype(int)\n",
        "\n",
        "    # Calculate removal stats\n",
        "    total_removed = len(df) - len(cleaned_df)\n",
        "    removal_rate = total_removed / len(df) * 100\n",
        "\n",
        "    print(f\"Total records removed: {total_removed} ({removal_rate:.2f}%)\\n\")\n",
        "    print(\"Record counts per class:\")\n",
        "    return count_df.sort_values(by='Populated Records', ascending=False)\n",
        "\n",
        "\n",
        "result_df = process_landmarks_dataset(\"/content/asl_landmarks_combined.csv\")\n",
        "print(result_df.to_string())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GA2qnWm3BQat"
      },
      "outputs": [],
      "source": [
        "result_df.to_csv('/content/asl_landmarks_combined.csv',index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qSmYonCyBrXE",
        "outputId": "4ccf1411-889c-4bd3-fc6f-bfb6a931f7c3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Index(['Original Records', 'Populated Records'], dtype='object')\n"
          ]
        }
      ],
      "source": [
        "print(pd.read_csv('/content/asl_landmarks_combined.csv').columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t9sq-zG7DnzD"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "csv_path = '/content/drive/MyDrive/asl_landmarks_combined.csv'\n",
        "df = pd.read_csv(csv_path)\n",
        "\n",
        "    # Identify landmark columns (all columns except 'label')\n",
        "landmark_cols = df.columns.drop('label')\n",
        "\n",
        "    # Find rows where ALL landmarks are missing\n",
        "all_missing_mask = df[landmark_cols].isna().all(axis=1)\n",
        "\n",
        "    # Create cleaned dataset\n",
        "cleaned_df = df[~all_missing_mask].copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "iMvahwB0HVPO",
        "outputId": "9b1dd868-63a8-4b50-87a4-796a8ee22776"
      },
      "outputs": [],
      "source": [
        "print(cleaned_df.tail())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "q0BqJldcAcEK",
        "outputId": "4d5b560d-ef9a-4d00-993d-76f2d4667d7c"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from imblearn.pipeline import Pipeline\n",
        "import numpy as np\n",
        "\n",
        "# Load cleaned data\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/asl_landmarks_cleaned.csv\")\n",
        "\n",
        "# 1. Remove 'nothing' class\n",
        "df = df[df['label'] != 'nothing']\n",
        "\n",
        "# 2. Separate features and labels\n",
        "X = df.drop('label', axis=1)\n",
        "y = df['label']\n",
        "\n",
        "# 3. Define sampling strategy\n",
        "class_counts = y.value_counts()\n",
        "\n",
        "sampling_strategy = {\n",
        "    class_name: (\n",
        "        2000 if count > 2000 else  # Undersample majority\n",
        "        500 if count < 500 else    # Oversample minority\n",
        "        count                      # Leave mid-tier\n",
        "    )\n",
        "    for class_name, count in class_counts.items()\n",
        "}\n",
        "\n",
        "# 4. Create pipeline\n",
        "pipeline = Pipeline([\n",
        "    ('oversample', SMOTE(\n",
        "        sampling_strategy={k: v for k, v in sampling_strategy.items() if v > class_counts[k]},\n",
        "        k_neighbors=2,  # Reduced for small classes\n",
        "        random_state=42\n",
        "    )),\n",
        "    ('undersample', RandomUnderSampler(\n",
        "        sampling_strategy={k: v for k, v in sampling_strategy.items() if v < class_counts[k]},\n",
        "        random_state=42\n",
        "    ))\n",
        "])\n",
        "\n",
        "# 5. Apply resampling\n",
        "X_res, y_res = pipeline.fit_resample(X, y)\n",
        "\n",
        "# 6. Add synthetic noise to oversampled minority classes\n",
        "minority_classes = [cls for cls, count in sampling_strategy.items() if count > class_counts[cls]]\n",
        "noise_scale = 0.02  # 2% of value range\n",
        "\n",
        "for cls in minority_classes:\n",
        "    mask = y_res == cls\n",
        "    X_res[mask] += np.random.normal(loc=0, scale=noise_scale, size=X_res[mask].shape)\n",
        "\n",
        "# 7. Save balanced dataset\n",
        "balanced_df = pd.concat([pd.DataFrame(X_res, columns=X.columns), pd.Series(y_res, name='label')], axis=1)\n",
        "balanced_df.to_csv(\"balanced_asl_dataset.csv\", index=False)\n",
        "\n",
        "# 8. Print new class distribution\n",
        "print(\"Balanced Class Counts:\")\n",
        "print(pd.Series(y_res).value_counts().sort_values(ascending=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "2kBo5XqDKOpM",
        "outputId": "6ba05373-ac21-439a-8633-50fa72a23375"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('/content/drive/MyDrive/asl_landmarks_balanced.csv')\n",
        "print(df.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y8k6eXUAOZw7",
        "outputId": "2425ff58-5d2c-4e3a-86cf-a4c4c99c20c2"
      },
      "outputs": [],
      "source": [
        "df = df.drop(df.columns[[0,1]],axis=1)\n",
        "print(df.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iUn4KgXHQVGo"
      },
      "outputs": [],
      "source": [
        "df.to_csv('/content/drive/MyDrive/asl_landmarks_balanced.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "COgSWonqYqlP"
      },
      "source": [
        "#To train an XGboost classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "8axLXG0_VFut",
        "outputId": "ee52fdd6-16b4-428b-cd36-2c8b49410520"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from xgboost import XGBClassifier\n",
        "import joblib\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv('/content/drive/MyDrive/asl_landmarks_balanced.csv')\n",
        "\n",
        "# Define feature columns: mediapipe landmarks from x_0,y_0,z_0 to x_20,y_20,z_20\n",
        "feature_columns = [f\"{axis}_{i}\" for i in range(21) for axis in ['x', 'y', 'z']]\n",
        "X = data[feature_columns]\n",
        "y = data['label']\n",
        "\n",
        "# Encode the string labels to integers for training\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(y)\n",
        "\n",
        "# Split the data into training and testing sets (stratify to preserve class distribution)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded\n",
        ")\n",
        "\n",
        "# Scale the features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Initialize and train the XGBoost classifier\n",
        "model = XGBClassifier(\n",
        "    objective='multi:softmax',\n",
        "    num_class=len(np.unique(y_encoded)),\n",
        "    eval_metric='mlogloss',  # multi-class log loss\n",
        "    use_label_encoder=False,\n",
        "    random_state=42\n",
        ")\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions (numeric)\n",
        "y_pred_numeric = model.predict(X_test)\n",
        "\n",
        "# Convert numeric predictions back to original string labels\n",
        "y_pred = label_encoder.inverse_transform(y_pred_numeric)\n",
        "y_test_str = label_encoder.inverse_transform(y_test)\n",
        "\n",
        "# Evaluate the model\n",
        "print(\"Accuracy:\", accuracy_score(y_test_str, y_pred))\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test_str, y_pred))\n",
        "\n",
        "# Save the model, scaler, and label encoder for deployment\n",
        "joblib.dump(model, \"xgb_classifier.pkl\")\n",
        "joblib.dump(scaler, \"scaler.pkl\")\n",
        "joblib.dump(label_encoder, \"label_encoder.pkl\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VT2iuVJfYyLe"
      },
      "source": [
        "#Testing the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1eBMOXwNY2sE",
        "outputId": "12f49bf8-1736-4212-c72d-ae76cdbc4bd2"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import mediapipe as mp\n",
        "import joblib\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# --- Load test CSV ---\n",
        "# Assume test.csv has a column \"filenames\" and the rest of the columns (one-hot) are the class labels.\n",
        "test_csv_path = \"/content/drive/MyDrive/ASL Dataset.v4-v1-aug-and-prep-with-validation-set.multiclass/test/_classes.csv\"\n",
        "test_df = pd.read_csv(test_csv_path)\n",
        "\n",
        "# Determine the one-hot encoded label columns (all columns except \"filenames\")\n",
        "onehot_columns = [col for col in test_df.columns if col != \"filenames\"]\n",
        "\n",
        "# Function to convert a one-hot encoded row to the corresponding label string.\n",
        "def one_hot_to_label(row):\n",
        "    for col in onehot_columns:\n",
        "        if row[col] == 1:\n",
        "            return col\n",
        "    return None\n",
        "\n",
        "test_df['true_label'] = test_df.apply(one_hot_to_label, axis=1)\n",
        "\n",
        "# --- Load saved model artifacts ---\n",
        "model = joblib.load(\"xgb_classifier.pkl\")\n",
        "scaler = joblib.load(\"scaler.pkl\")\n",
        "label_encoder = joblib.load(\"label_encoder.pkl\")\n",
        "\n",
        "# --- Initialize MediaPipe Hands ---\n",
        "mp_hands = mp.solutions.hands\n",
        "hands = mp_hands.Hands(\n",
        "    static_image_mode=True,\n",
        "    max_num_hands=1,\n",
        "    min_detection_confidence=0.5\n",
        ")\n",
        "\n",
        "# --- Process each test image ---\n",
        "predictions = []\n",
        "ground_truths = []\n",
        "for idx, row in test_df.iterrows():\n",
        "    filename = row['filename']\n",
        "    true_label = row['true_label']\n",
        "\n",
        "    # Construct full file path (assumes images are in content/test/)\n",
        "    filepath = os.path.join(\"/content/drive/MyDrive/ASL Dataset.v4-v1-aug-and-prep-with-validation-set.multiclass/test/\", filename)\n",
        "\n",
        "    # Load image using cv2 (BGR format)\n",
        "    image = cv2.imread(filepath)\n",
        "    if image is None:\n",
        "        print(f\"Error reading image: {filepath}\")\n",
        "        continue\n",
        "\n",
        "    # Convert to RGB as required by MediaPipe\n",
        "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    # Process the image to detect hand landmarks\n",
        "    results = hands.process(image_rgb)\n",
        "    if not results.multi_hand_landmarks:\n",
        "        print(f\"No hand landmarks detected for image: {filepath}\")\n",
        "        continue\n",
        "\n",
        "    # Use the first detected hand\n",
        "    hand_landmarks = results.multi_hand_landmarks[0]\n",
        "\n",
        "    # Extract features: we expect 21 landmarks with x, y, z each -> 63 features\n",
        "    features = []\n",
        "    for landmark in hand_landmarks.landmark:\n",
        "        features.extend([landmark.x, landmark.y, landmark.z])\n",
        "\n",
        "    if len(features) != 63:\n",
        "        print(f\"Unexpected number of landmarks for image: {filepath}\")\n",
        "        continue\n",
        "\n",
        "    # Prepare features for prediction\n",
        "    features = np.array(features).reshape(1, -1)\n",
        "    features_scaled = scaler.transform(features)\n",
        "\n",
        "    # Predict using the trained model (model expects numeric labels)\n",
        "    pred_numeric = model.predict(features_scaled)\n",
        "    pred_label = label_encoder.inverse_transform(pred_numeric)[0]\n",
        "\n",
        "    predictions.append(pred_label)\n",
        "    ground_truths.append(true_label)\n",
        "\n",
        "# Clean up MediaPipe resources\n",
        "hands.close()\n",
        "\n",
        "# --- Evaluate predictions ---\n",
        "if ground_truths:\n",
        "    acc = accuracy_score(ground_truths, predictions)\n",
        "    report = classification_report(ground_truths, predictions)\n",
        "    print(\"Test Accuracy:\", acc)\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(report)\n",
        "else:\n",
        "    print(\"No valid predictions were made. Check if the test images are processed correctly.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RW3CrtdBV2m5"
      },
      "source": [
        "#Data processing \n",
        "test.csv has filename and one hot encoded labels, change it to absolute path and string label to make test easier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NLbuCCY2VzFy",
        "outputId": "c54f1144-4868-4280-f12a-b35b7faafb56"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "def process_csv(input_file, output_file):\n",
        "    \"\"\"Processes CSV with one-hot labels to simpler format.\"\"\"\n",
        "\n",
        "    try:\n",
        "        df = pd.read_csv(input_file)\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: Input file '{input_file}' not found.\")\n",
        "        return\n",
        "\n",
        "    df['filepath'] = '/content/drive/MyDrive/ASL Dataset.v4-v1-aug-and-prep-with-validation-set.multiclass/test/' + df['filename']\n",
        "    df.drop(columns=['filename'], inplace=True)\n",
        "    label_cols = df.columns[1:]  # Exclude filepath\n",
        "\n",
        "    # More robust way to find the label:\n",
        "    df['label'] = ''  # Initialize an empty label column\n",
        "    for index, row in df.iterrows():  # Iterate through each row\n",
        "        for col in label_cols:\n",
        "            if row[col] == 1:\n",
        "                df.loc[index, 'label'] = col  # Assign correct label string\n",
        "                break  # Stop checking once label is found\n",
        "    df.drop(columns=df.columns[:-2], inplace=True)\n",
        "    print(df)\n",
        "    #df = df[['filepath', 'label']]\n",
        "\n",
        "    try:\n",
        "        df.to_csv(output_file, index=False)\n",
        "        print(f\"Processed '{input_file}' and saved to '{output_file}'.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error writing to output file: {e}\")\n",
        "\n",
        "\n",
        "\n",
        "# Example usage:\n",
        "input_csv = \"/content/drive/MyDrive/ASL Dataset.v4-v1-aug-and-prep-with-validation-set.multiclass/test/_classes.csv\"\n",
        "output_csv = \"/content/drive/MyDrive/ASL Dataset.v4-v1-aug-and-prep-with-validation-set.multiclass/test/classes_procesed.csv\"\n",
        "process_csv(input_csv, output_csv)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f57WntN5U2vx"
      },
      "source": [
        "#Another model training\n",
        "The previous one did not have good test accuracy. Take landmark x_0, y_0 and z_0 (wrist) as origin and take the relative position to train the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "CSfXAwTZVTmc",
        "outputId": "d28d689b-d3f0-413c-d1c4-607bf79bd997"
      },
      "outputs": [],
      "source": [
        "# ====================\n",
        "# TRAINING PIPELINE\n",
        "# ====================\n",
        "import cv2\n",
        "import mediapipe as mp\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import joblib\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# 1. Load and preprocess training data\n",
        "df = pd.read_csv('/content/drive/MyDrive/asl_landmarks_balanced.csv')\n",
        "\n",
        "\"\"\"\n",
        "There is a weird bug where colab saves the index as 'Unnamed'\n",
        "This will mess up the shape so if it is saved as such, drop that column\n",
        "\"\"\"\n",
        "if df.columns[0]=='Unnamed: 0':\n",
        "  df.drop(columns=df.columns[0],inplace = True)\n",
        "\n",
        "# Convert absolute landmarks to relative (wrist as origin)\n",
        "def convert_to_relative(landmarks):\n",
        "    landmarks = np.array(landmarks).reshape(-1, 21, 3)\n",
        "    wrist = landmarks[:, 0, :]\n",
        "    relative = landmarks - wrist[:, np.newaxis, :]\n",
        "    return relative[:, 1:, :].reshape(-1, 60)  # Remove wrist (now 0,0,0)\n",
        "\n",
        "X_absolute = df.drop('label', axis=1).values\n",
        "X = convert_to_relative(X_absolute)\n",
        "y = df['label'].values\n",
        "\n",
        "# 2. Encode labels\n",
        "le = LabelEncoder()\n",
        "y_encoded = le.fit_transform(y)\n",
        "\n",
        "# 3. Split and scale\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y_encoded, test_size=0.2, stratify=y_encoded, random_state=42\n",
        ")\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# 4. Train model\n",
        "model = XGBClassifier(\n",
        "    n_estimators=1000,\n",
        "    learning_rate=0.05,\n",
        "    max_depth=7,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8,\n",
        "    tree_method='hist',\n",
        "    eval_metric='mlogloss',\n",
        "    early_stopping_rounds=50,\n",
        "    use_label_encoder=False\n",
        ")\n",
        "\n",
        "model.fit(X_train_scaled, y_train,\n",
        "          eval_set=[(X_test_scaled, y_test)],\n",
        "          verbose=True)\n",
        "\n",
        "# 5. Save artifacts\n",
        "joblib.dump(scaler, 'scaler_relative.pkl')\n",
        "joblib.dump(le, 'label_encoder_relative.pkl')\n",
        "model.save_model('asl_model_relative.json')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rZAubviBoMW8"
      },
      "source": [
        "#For model inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c582ghJ-oHO1"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import mediapipe as mp\n",
        "import joblib\n",
        "import xgboost as xgb\n",
        "import numpy as np\n",
        "from sklearn.base import BaseEstimator, ClassifierMixin\n",
        "\n",
        "class XGBoostCompatWrapper(BaseEstimator, ClassifierMixin):\n",
        "    \"\"\"Wrapper to ensure full sklearn compatibility\"\"\"\n",
        "    def __init__(self, model_path):\n",
        "        self.model = xgb.Booster()\n",
        "        self.model.load_model(model_path)\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        dmat = xgb.DMatrix(X)\n",
        "        return self.model.predict(dmat)\n",
        "\n",
        "    def predict(self, X):\n",
        "        return np.argmax(self.predict_proba(X), axis=1)\n",
        "\n",
        "class ASLClassifier:\n",
        "    def __init__(self):\n",
        "        # Initialize MediaPipe\n",
        "        self.hands = mp.solutions.hands.Hands(\n",
        "            static_image_mode=True,\n",
        "            max_num_hands=2,\n",
        "            min_detection_confidence=0.5\n",
        "        )\n",
        "\n",
        "        # Load preprocessing artifacts\n",
        "        self.scaler = joblib.load('scaler_relative.pkl')\n",
        "        self.le = joblib.load('label_encoder_relative.pkl')\n",
        "\n",
        "        # Load XGBoost model with compatibility wrapper\n",
        "        self.model = XGBoostCompatWrapper('asl_model_relative.json')\n",
        "\n",
        "    def _process_landmarks(self, landmarks):\n",
        "        \"\"\"Convert absolute landmarks to relative format\"\"\"\n",
        "        landmarks = np.array(landmarks).reshape(1, 21, 3)\n",
        "        wrist = landmarks[:, 0, :]\n",
        "        relative = landmarks - wrist\n",
        "        return relative[:, 1:, :].reshape(1, 60)\n",
        "\n",
        "    def predict(self, image_path):\n",
        "        try:\n",
        "            # Load and process image\n",
        "            image = cv2.imread(image_path)\n",
        "            if image is None:\n",
        "                return \"Invalid image\", 0.0\n",
        "\n",
        "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "            results = self.hands.process(image)\n",
        "\n",
        "            if not results.multi_hand_landmarks:\n",
        "                return \"No hands detected\", 0.0\n",
        "\n",
        "            # Extract landmarks\n",
        "            raw_landmarks = []\n",
        "            for landmark in results.multi_hand_landmarks[0].landmark:\n",
        "                raw_landmarks.extend([landmark.x, landmark.y, landmark.z])\n",
        "\n",
        "            if len(raw_landmarks) != 63:\n",
        "                return \"Invalid landmarks\", 0.0\n",
        "\n",
        "            # Process and predict\n",
        "            processed = self._process_landmarks(raw_landmarks)\n",
        "            scaled = self.scaler.transform(processed)\n",
        "\n",
        "            proba = self.model.predict_proba(scaled)[0]\n",
        "            pred_class = self.le.inverse_transform([np.argmax(proba)])[0]\n",
        "            return pred_class, np.max(proba)\n",
        "\n",
        "        except Exception as e:\n",
        "            return f\"Error: {str(e)}\", 0.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mJRvgemloMp7"
      },
      "source": [
        "#Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sKuchjRPooDa",
        "outputId": "7fe98a0c-6167-42e5-83eb-382dbbd6a51f"
      },
      "outputs": [],
      "source": [
        "# Load test data\n",
        "test_df = pd.read_csv('/content/drive/MyDrive/ASL Dataset.v4-v1-aug-and-prep-with-validation-set.multiclass/test/classes_procesed.csv')\n",
        "\n",
        "# Initialize classifier\n",
        "classifier = ASLClassifier()\n",
        "\n",
        "# Process test images\n",
        "y_true = []\n",
        "y_pred = []\n",
        "confidences = []\n",
        "\n",
        "for _, row in test_df.iterrows():\n",
        "    pred, conf = classifier.predict(row['filepath'])\n",
        "    y_true.append(row['label'])\n",
        "    y_pred.append(pred)\n",
        "    confidences.append(conf)\n",
        "\n",
        "# Generate report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_true, y_pred,))\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = np.mean(np.array(y_true) == np.array(y_pred))\n",
        "print(f\"\\nOverall Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Average Confidence: {np.mean(confidences):.4f}\")\n",
        "\n",
        "# Error analysis\n",
        "error_mask = np.array(y_true) != np.array(y_pred)\n",
        "print(\"\\nTop Misclassified Examples:\")\n",
        "for path, true, pred in zip(test_df[error_mask]['filepath'],\n",
        "                            np.array(y_true)[error_mask],\n",
        "                            np.array(y_pred)[error_mask]):\n",
        "    print(f\"{path}: {true} -> {pred}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
